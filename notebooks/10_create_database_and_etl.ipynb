{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8b9223",
   "metadata": {},
   "source": [
    "Code used to create tables is in src/utils/create_db.sql file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6dda05",
   "metadata": {},
   "source": [
    "This notebook does etl to db in Cloud SQL, as a source of data it takes top_songs_curated.csv from data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91221d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1de7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# --- Database Connection Parameters ---\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"database-instance\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "\n",
    "# --- CSV File Path ---\n",
    "CSV_FILE_PATH = \"../data/top_songs_curated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce066bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72f3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    \"\"\"\n",
    "    Parses a date string into a datetime.date object.\n",
    "    Handles empty or invalid date strings by returning None.\n",
    "    \"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    try:\n",
    "        if len(date_str) == 10:  # YYYY-MM-DD\n",
    "            return datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not parse date string: {date_str}. Storing as NULL.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fadbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection():\n",
    "    \"\"\"Establishes and returns a database connection.\"\"\"\n",
    "    logging.info(\n",
    "        f\"Attempting to connect to database '{DB_NAME}' on {DB_HOST}:{DB_PORT}...\"\n",
    "    )\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD,\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "        )\n",
    "        logging.info(\"Successfully connected to the database.\")\n",
    "        return conn\n",
    "    except psycopg2.OperationalError as e:\n",
    "        logging.error(f\"Database connection failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64302858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_and_load_data(conn, csv_filepath):\n",
    "    \"\"\"\n",
    "    Reads the CSV file, processes each row, and loads data into the database.\n",
    "\n",
    "    Args:\n",
    "        conn: A psycopg2 database connection object.\n",
    "        csv_filepath (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting ETL process for CSV: {csv_filepath}\")\n",
    "    processed_rows = 0\n",
    "    inserted_tracks = 0\n",
    "    updated_tracks = 0\n",
    "    inserted_genres = 0\n",
    "    linked_track_genres = 0\n",
    "\n",
    "    required_cols = [\n",
    "        \"Track URI\",\n",
    "        \"Track Name\",\n",
    "        \"Artist Name(s)\",\n",
    "        \"Album Name\",\n",
    "        \"Album Release Date\",\n",
    "        \"Album Image URL\",\n",
    "        \"Track Duration (ms)\",\n",
    "        \"Explicit\",\n",
    "        \"Popularity\",\n",
    "        \"Artist Genres\",\n",
    "        \"youtube_title\",\n",
    "        \"youtube_url\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with open(csv_filepath, mode=\"r\", encoding=\"utf-8\") as csvfile:\n",
    "            csv_reader = csv.DictReader(csvfile)\n",
    "\n",
    "            header = csv_reader.fieldnames\n",
    "            if not header:\n",
    "                logging.error(\"CSV file is empty or has no header.\")\n",
    "                return\n",
    "\n",
    "            missing_cols = [col for col in required_cols if col not in header]\n",
    "            if missing_cols:\n",
    "                logging.error(\n",
    "                    f\"Missing required columns in CSV header: {', '.join(missing_cols)}\"\n",
    "                )\n",
    "                logging.info(f\"Available columns: {header}\")\n",
    "                return\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                for row_num, row in enumerate(csv_reader, 1):\n",
    "                    try:\n",
    "                        # 1. Extract and prepare track data\n",
    "                        original_track_uri = row.get(\"Track URI\")\n",
    "                        track_name = row.get(\"Track Name\")\n",
    "\n",
    "                        if not original_track_uri:\n",
    "                            logging.warning(\n",
    "                                f\"Skipping row {row_num} due to missing 'Track URI'.\"\n",
    "                            )\n",
    "                            continue\n",
    "                        if not track_name:\n",
    "                            logging.warning(\n",
    "                                f\"Skipping row {row_num} (URI: {original_track_uri}) due to missing 'Track Name'.\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        artist_names = row.get(\"Artist Name(s)\")\n",
    "                        album_name = row.get(\"Album Name\")\n",
    "                        album_release_date_str = row.get(\"Album Release Date\")\n",
    "                        album_image_url = row.get(\"Album Image URL\")\n",
    "\n",
    "                        track_duration_ms_str = row.get(\"Track Duration (ms)\")\n",
    "                        track_duration_ms = (\n",
    "                            int(track_duration_ms_str)\n",
    "                            if track_duration_ms_str and track_duration_ms_str.isdigit()\n",
    "                            else None\n",
    "                        )\n",
    "\n",
    "                        explicit_str = row.get(\"Explicit\", \"\").strip().lower()\n",
    "                        explicit = explicit_str == \"true\" if explicit_str else None\n",
    "\n",
    "                        popularity_str = row.get(\"Popularity\")\n",
    "                        popularity = (\n",
    "                            int(popularity_str)\n",
    "                            if popularity_str and popularity_str.isdigit()\n",
    "                            else None\n",
    "                        )\n",
    "\n",
    "                        youtube_title = row.get(\"youtube_title\")\n",
    "                        youtube_url = row.get(\"youtube_url\")\n",
    "                        artist_genres_str = row.get(\"Artist Genres\", \"\")\n",
    "\n",
    "                        album_release_date = parse_date(album_release_date_str)\n",
    "\n",
    "                        # 2. Insert/Update Track and get track_id\n",
    "                        track_sql = sql.SQL(\n",
    "                            \"\"\"\n",
    "                            INSERT INTO tracks (\n",
    "                                original_track_uri, track_name, artist_names, album_name,\n",
    "                                album_release_date, album_image_url, track_duration_ms,\n",
    "                                explicit, popularity, youtube_title, youtube_url\n",
    "                            ) VALUES (\n",
    "                                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
    "                            )\n",
    "                            ON CONFLICT (original_track_uri) DO UPDATE SET\n",
    "                                track_name = EXCLUDED.track_name,\n",
    "                                artist_names = EXCLUDED.artist_names,\n",
    "                                album_name = EXCLUDED.album_name,\n",
    "                                album_release_date = EXCLUDED.album_release_date,\n",
    "                                album_image_url = EXCLUDED.album_image_url,\n",
    "                                track_duration_ms = EXCLUDED.track_duration_ms,\n",
    "                                explicit = EXCLUDED.explicit,\n",
    "                                popularity = EXCLUDED.popularity,\n",
    "                                youtube_title = EXCLUDED.youtube_title,\n",
    "                                youtube_url = EXCLUDED.youtube_url,\n",
    "                                updated_at = CURRENT_TIMESTAMP\n",
    "                            RETURNING track_id, (xmax = 0) AS inserted;\n",
    "                        \"\"\"\n",
    "                        )\n",
    "\n",
    "                        cursor.execute(\n",
    "                            track_sql,\n",
    "                            (\n",
    "                                original_track_uri,\n",
    "                                track_name,\n",
    "                                artist_names,\n",
    "                                album_name,\n",
    "                                album_release_date,\n",
    "                                album_image_url,\n",
    "                                track_duration_ms,\n",
    "                                explicit,\n",
    "                                popularity,\n",
    "                                youtube_title,\n",
    "                                youtube_url,\n",
    "                            ),\n",
    "                        )\n",
    "                        track_id_result = cursor.fetchone()\n",
    "                        if not track_id_result:\n",
    "                            logging.error(\n",
    "                                f\"Failed to insert or update track (URI: {original_track_uri}). Skipping genre processing for this track.\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        track_id, was_inserted = track_id_result\n",
    "                        if was_inserted:\n",
    "                            inserted_tracks += 1\n",
    "                        else:\n",
    "                            updated_tracks += 1\n",
    "                        # logging.info(f\"Processed track ID: {track_id} (URI: {original_track_uri}) - {'Inserted' if was_inserted else 'Updated'}\")\n",
    "\n",
    "                        # 3. Process and Insert Genres and Track-Genre links\n",
    "                        if artist_genres_str:\n",
    "                            genres_from_csv = [\n",
    "                                genre.strip()\n",
    "                                for genre in artist_genres_str.split(\",\")\n",
    "                                if genre.strip()\n",
    "                            ]\n",
    "                            for genre_name in genres_from_csv:\n",
    "                                if not genre_name:\n",
    "                                    continue\n",
    "\n",
    "                                # a. Insert Genre if not exists, and get genre_id\n",
    "                                genre_id = None\n",
    "                                cursor.execute(\n",
    "                                    sql.SQL(\n",
    "                                        \"INSERT INTO genres (genre_name) VALUES (%s) ON CONFLICT (genre_name) DO NOTHING RETURNING genre_id;\"\n",
    "                                    ),\n",
    "                                    (genre_name,),\n",
    "                                )\n",
    "                                result = cursor.fetchone()\n",
    "                                if result:\n",
    "                                    genre_id = result[0]\n",
    "                                    inserted_genres += 1\n",
    "                                    # logging.info(f\"Inserted new genre: '{genre_name}' with ID: {genre_id}\")\n",
    "                                else:  # Genre already existed, fetch its ID\n",
    "                                    cursor.execute(\n",
    "                                        sql.SQL(\n",
    "                                            \"SELECT genre_id FROM genres WHERE genre_name = %s;\"\n",
    "                                        ),\n",
    "                                        (genre_name,),\n",
    "                                    )\n",
    "                                    result = cursor.fetchone()\n",
    "                                    if result:\n",
    "                                        genre_id = result[0]\n",
    "                                    else:\n",
    "                                        logging.error(\n",
    "                                            f\"Could not find or insert genre: '{genre_name}' for track ID {track_id}. Skipping this genre link.\"\n",
    "                                        )\n",
    "                                        continue\n",
    "\n",
    "                                # b. Insert Track-Genre link\n",
    "                                try:\n",
    "                                    cursor.execute(\n",
    "                                        sql.SQL(\n",
    "                                            \"INSERT INTO track_genres (track_id, genre_id) VALUES (%s, %s) ON CONFLICT (track_id, genre_id) DO NOTHING;\"\n",
    "                                        ),\n",
    "                                        (track_id, genre_id),\n",
    "                                    )\n",
    "                                    if (\n",
    "                                        cursor.rowcount > 0\n",
    "                                    ):  # rowcount is 1 if inserted, 0 if conflict and did nothing\n",
    "                                        linked_track_genres += 1\n",
    "                                        # logging.info(f\"Linked track ID {track_id} to genre ID {genre_id} ('{genre_name}')\")\n",
    "                                except psycopg2.Error as link_err:\n",
    "                                    logging.error(\n",
    "                                        f\"Error linking track ID {track_id} to genre ID {genre_id} ('{genre_name}'): {link_err}\"\n",
    "                                    )\n",
    "\n",
    "                        processed_rows += 1\n",
    "                        if processed_rows % 100 == 0:  # Log progress every 100 rows\n",
    "                            logging.info(f\"Processed {processed_rows} rows...\")\n",
    "                            conn.commit()  # Commit periodically for large files\n",
    "\n",
    "                    except psycopg2.Error as db_err:\n",
    "                        logging.error(\n",
    "                            f\"Database error processing row {row_num} (Track URI: {row.get('Track URI', 'N/A')}): {db_err}\"\n",
    "                        )\n",
    "                        conn.rollback()  # Rollback current transaction segment\n",
    "                        # Decide if you want to continue with the next row or stop\n",
    "                    except Exception as e:\n",
    "                        logging.error(\n",
    "                            f\"General error processing row {row_num} (Track URI: {row.get('Track URI', 'N/A')}): {e}\"\n",
    "                        )\n",
    "                        conn.rollback()  # Rollback current transaction segment\n",
    "\n",
    "                conn.commit()  # Final commit for any remaining operations\n",
    "                logging.info(\"ETL process completed.\")\n",
    "                logging.info(f\"Summary: Processed {processed_rows} rows from CSV.\")\n",
    "                logging.info(\n",
    "                    f\"Tracks: {inserted_tracks} inserted, {updated_tracks} updated.\"\n",
    "                )\n",
    "                logging.info(\n",
    "                    f\"Genres: {inserted_genres} newly inserted (others may have existed).\"\n",
    "                )\n",
    "                logging.info(f\"Track-Genre Links: {linked_track_genres} created.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"The file '{csv_filepath}' was not found.\")\n",
    "    except (\n",
    "        psycopg2.Error\n",
    "    ) as e:  # Catch errors related to initial connection or cursor creation\n",
    "        logging.error(f\"A database error occurred during ETL setup: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()  # Ensure rollback if error happens before main loop\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the ETL process: {e}\")\n",
    "        if conn:  # Check if conn was successfully initialized\n",
    "            conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0158689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 17:59:00,844 - INFO - Starting Python ETL script...\n",
      "2025-05-19 17:59:00,846 - INFO - Attempting to connect to database 'database-instance' on 34.140.62.43:5432...\n",
      "2025-05-19 17:59:01,539 - INFO - Successfully connected to the database.\n",
      "2025-05-19 17:59:01,540 - INFO - Starting ETL process for CSV: ../data/top_songs_curated.csv\n",
      "2025-05-19 17:59:36,632 - INFO - Processed 100 rows...\n",
      "2025-05-19 18:00:12,814 - INFO - Processed 200 rows...\n",
      "2025-05-19 18:00:46,738 - INFO - Processed 300 rows...\n",
      "2025-05-19 18:01:34,232 - INFO - Processed 400 rows...\n",
      "2025-05-19 18:02:01,890 - INFO - Processed 500 rows...\n",
      "2025-05-19 18:02:32,500 - INFO - Processed 600 rows...\n",
      "2025-05-19 18:02:57,848 - INFO - Processed 700 rows...\n",
      "2025-05-19 18:03:32,180 - INFO - Processed 800 rows...\n",
      "2025-05-19 18:04:10,520 - INFO - Processed 900 rows...\n",
      "2025-05-19 18:05:00,598 - INFO - Processed 1000 rows...\n",
      "2025-05-19 18:05:26,615 - INFO - ETL process completed.\n",
      "2025-05-19 18:05:26,618 - INFO - Summary: Processed 1070 rows from CSV.\n",
      "2025-05-19 18:05:26,619 - INFO - Tracks: 1070 inserted, 0 updated.\n",
      "2025-05-19 18:05:26,620 - INFO - Genres: 125 newly inserted (others may have existed).\n",
      "2025-05-19 18:05:26,620 - INFO - Track-Genre Links: 3520 created.\n",
      "2025-05-19 18:05:26,630 - INFO - Database connection closed.\n",
      "2025-05-19 18:05:26,634 - INFO - Python ETL script finished.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting Python ETL script...\")\n",
    "\n",
    "# Validate essential configurations\n",
    "if DB_NAME == \"your_db_name\" or DB_USER == \"your_db_user\":\n",
    "    logging.error(\n",
    "        \"CRITICAL: Default database credentials are still in use. Please update DB_NAME, DB_USER, and DB_PASSWORD.\"\n",
    "    )\n",
    "    exit(1)\n",
    "if CSV_FILE_PATH == \"your_spotify_data.csv\" and not os.path.exists(CSV_FILE_PATH):\n",
    "    logging.warning(\n",
    "        f\"Default CSV_FILE_PATH '{CSV_FILE_PATH}' is set. Make sure this file exists or update the path.\"\n",
    "    )\n",
    "    # You might want to exit here if the default path is unlikely to be correct and doesn't exist.\n",
    "    # For now, we'll let it proceed and fail in process_csv_and_load_data if not found.\n",
    "\n",
    "connection = None\n",
    "try:\n",
    "    connection = get_db_connection()\n",
    "    if connection:\n",
    "        process_csv_and_load_data(connection, CSV_FILE_PATH)\n",
    "except Exception as e:\n",
    "    # This will catch connection errors from get_db_connection or any other unhandled exceptions\n",
    "    logging.critical(f\"ETL process failed critically: {e}\")\n",
    "finally:\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        logging.info(\"Database connection closed.\")\n",
    "logging.info(\"Python ETL script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asseco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
